#! -*- coding:utf-8 -*-
'''

Model graph module

@author: LouisZBravo

'''
import config as cfg
from layers import MaxOnASeqLayer, SumScoreLayer, AttentionMatrixLayer, L2NormLayer, SumByAxisLayer
from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dot, Concatenate, Dense, Dropout, Multiply, TimeDistributed, Softmax
from keras.layers import Permute, Reshape
from keras import regularizers

class GatedSimMatrixModelGraph(object):
    '''
     word-level GLU mechanism based on similarity matrix training
     this model graph can be used as an independent keras model, as well as part of another model.
     arguments:
         _q_input, _a_input: tensor unit, sentence matrices of (sent_len, wdim)
         _q_rcprcl_len_input, _a_rcprcl_len_input: tensor unit, reciprocal length of sentence of (1,)
         indep_model: boolean, use this class as an independent keras model, with inputs and outputs defined
         trainable: boolean, 
         
     as an independent model, it takes input from keras backend. it generates matching score of the sentence pair. see get_model_in/outputs
     as a part of a model, it takes input from arguments. it provides matching scores in get_score_graph_unit
     it generates matching score only
     for gated sentence matrix in word vectors, see GatedWordRepModelGraph below
    '''
    def __init__(self, wdim, _q_input = None, _a_input = None, _q_rcprcl_len_input = None, _a_rcprcl_len_input = None, 
                 indep_model = False, trainable = True, attention_based = False):
        '''
         when indep_model is False, graph input arguments shouldn't be None.
        '''
        if indep_model:
            self.is_model = True
            _q_input = Input(shape=(None, wdim))
            _a_input = Input(shape=(None, wdim))
            _q_rcprcl_len_input = Input(shape=(1,))
            _a_rcprcl_len_input = Input(shape=(1,))
            self.graph_input_units = (_q_input, _a_input, _q_rcprcl_len_input, _a_rcprcl_len_input)
            score_unit_and_len_tuple = self._model_graph(wdim, _q_input, _a_input, _q_rcprcl_len_input, _a_rcprcl_len_input)
            assert score_unit_and_len_tuple[1] == 1 # currently only one dim output is supported
            self.graph_output_units = score_unit_and_len_tuple[0]
        else:
            self.is_model = False
            score_unit_and_len_tuple = self._model_graph(wdim, _q_input, _a_input, _q_rcprcl_len_input, _a_rcprcl_len_input)
            self.score_unit = score_unit_and_len_tuple[0]
            self.score_unit_len = score_unit_and_len_tuple[1]
        self.weight_mtx_num = cfg.ModelConfig.PERSPECTIVES
    
    def _model_graph(self, wdim, _q_input, _a_input, _q_rcprcl_len_input, _a_rcprcl_len_input):
        '''
         define the similarity-matrix-based task adaptive GLU mechanism
         return a tuple:
            1. a tensor of score(s) generated by the mechanism
            2. length of score tensor
        '''
        perspectives = cfg.ModelConfig.PERSPECTIVES
        self._word_gate_lst = []
        score_item_lst = []
        # multi-perspective version of the above mechanism
        for i in range(perspectives):
            self._word_gate_lst.append(Dense(units = wdim,
                                       activation = 'sigmoid',
                                       use_bias = True,
                                       ))
            gate_layer = TimeDistributed(self._word_gate_lst[i])
            _q_gate = gate_layer(_q_input) # (*perspectives*, sent len, wdim), same gate
            _a_gate = gate_layer(_a_input)
            _gated_q = Multiply()([_q_input, _q_gate]) # (*perspectives*, sent len, wdim)
            _gated_a = Multiply()([_a_input, _a_gate])
            # match mechanism
            #_normed_q = L2NormLayer()(_q_input) # (*perspectives*, sent len, wdim)
            #_normed_a = L2NormLayer()(_a_input)
            _normed_q = L2NormLayer()(_gated_q) # (*perspectives*, sent len, wdim)
            _normed_a = L2NormLayer()(_gated_a)
            # match mechanism specified similarity matrix
            _sim_mtx = Dot(axes = -1)([_normed_q, _normed_a]) # (*perspectives*, q_sent_len, a_sent_len)
            # attention bag-of-word
            #_q_atten_mtx = Softmax(axis = -2)(_sim_mtx) # (*perspectives*, q_len, a_len)
            _a_atten_mtx = Permute((2, 1))(Softmax(axis = -1)(_sim_mtx)) # (*perspectives*, a_len, q_len), a transposed sim matrix softmaxed along a sent
            #_q_to_a_contxt_mtx = Dot(axes = -2)([_q_atten_mtx, _q_input]) # (*perspectives*, a_sent_len, wdim), per-feature weighed sum(dot), weighed bag-of-words
            _a_to_q_contxt_mtx = Dot(axes = -2)([_a_atten_mtx, _a_input]) # (*perspectives*, q_sent_len, wdim)
            # attention similarity matching
            #_normed_q_to_a_contxt_mtx = L2NormLayer()(_q_to_a_contxt_mtx)
            _normed_a_to_q_contxt_mtx = L2NormLayer()(_a_to_q_contxt_mtx)
            #_atten_q_to_a_match_res_mtx = Multiply()([_normed_q_to_a_contxt_mtx, _normed_a]) # (*perspectives*, a_sent_len, wdim)
            _atten_a_to_q_match_res_mtx = Multiply()([_normed_a_to_q_contxt_mtx, _normed_q]) # (*perspectives*, q_sent_len, wdim)
            # scoring
            # word level
            _q_words_best_match = MaxOnASeqLayer()(_sim_mtx) # (*perspectives*, q_sent_len)
            #_a_words_best_match = MaxOnASeqLayer()(_sim_mtx) # disabled, proved to be disfavoring
            _sum_along_q = SumScoreLayer()(_q_words_best_match) # (1, )
            #_sum_along_a = SumScoreLayer()(_a_words_best_match) 
            score_item_lst.append(Multiply()([_sum_along_q, _q_rcprcl_len_input])) # (*perspectives*, 1), average on q word-level
            # attention context level
            #_atten_a_match_score = SumByAxisLayer(axis = -1, keepdims = False)(_atten_q_to_a_match_res_mtx) # (*perspectives*, q_sent_len)
            _atten_q_match_score = SumByAxisLayer(axis = -1, keepdims = False)(_atten_a_to_q_match_res_mtx) # (*perspectives*, q_sent_len)
            #_sum_along_a_atten = SumScoreLayer()(_atten_a_match_score) # (*perspectives*, 1)
            _sum_along_q_atten = SumScoreLayer()(_atten_q_match_score) 
            #score_item_lst.append(Multiply()([_sum_along_q_atten, _q_rcprcl_len_input])) # (*perspectives*, 1), average on a (attentive) word-level
        if len(score_item_lst) > 1:
            return (Concatenate()(score_item_lst), len(score_item_lst))
        else:
            return (score_item_lst[0], 1)
    
    def get_model_inputs(self):
        return self.graph_input_units
    def get_model_outputs(self):
        return self.graph_output_units
    def get_score_graph_unit(self):
        '''
         first element of ret value: a keras tensor of score(s)
         second element: tensor length
        '''
        return (self.score_unit, self.score_unit_len)
    def set_gate_weights(self, saved_weight_lst):
        assert len(saved_weight_lst) == self.weight_mtx_num
        n = self.weight_mtx_num
        for i in range(n):
            self._word_gate_lst[i].set_weights(saved_weight_lst[i])
    
    def get_gate_weights(self):
        '''
         return a (list of) weights
         index: perspectives(one)
         elements: layer weight of keras, see keras.engine.topology
        '''
        n = self.weight_mtx_num
        weight_lst = []
        for i in range(n):
            cur_weights = self._word_gate_lst[i].get_weights()
            weight_lst.append(cur_weights)
        return weight_lst
        
class GatedWordRepModelGraph(object):
    '''
     graph for an independent model translating sentence matrix into gated matrix of the same size
    '''
    def __init__(self, wdim, weights):
        _s_input = Input(shape=(None, wdim))
        gate_layer = Dense(units = wdim,
                           activation = 'sigmoid',
                           use_bias = True,
                           )
        gate_layer.set_weights(weights)
        _s_input_gate = TimeDistributed(gate_layer)(_s_input)
        _gated_s = Multiply()([_s_input_gate, _s_input])
        self.graph_input_units = (_s_input,)
        self.graph_output_units = (_gated_s,)
    def get_model_inputs(self):
        return self.graph_input_units
    def get_model_outputs(self):
        return self.graph_output_units

class ConvQAModelGraph(object):
    def __init__(self, wdim):
        # hyperparameters
        conv_filter_len_siam = cfg.ModelConfig.CONV_FILTER_LEN_SIAM
        feat_map_num_siam = cfg.ModelConfig.FEATURE_MAP_NUM_SIAM
        #conv_filter_len_1 = cfg.ModelConfig.CONV_FILTER_LEN_1
        #feat_map_num_1 = cfg.ModelConfig.FEATURE_MAP_NUM_1
        #conv_filter_len_2 = cfg.ModelConfig.CONV_FILTER_LEN_2
        #feat_map_num_2 = cfg.ModelConfig.FEATURE_MAP_NUM_2
        
        # input dim: (sentence length, word dim)
        _q_input = Input(shape=(None, wdim))
        _a_input = Input(shape=(None, wdim))
        _q_rcprcl_len_input = Input(shape=(1,))
        _a_rcprcl_len_input = Input(shape=(1,))
        _add_feat_input = Input(shape=(4,))
        self.graph_input_units = (_q_input, _a_input, _q_rcprcl_len_input, _a_rcprcl_len_input, _add_feat_input)
        
        self.gated_sim_mtx_graph = GatedSimMatrixModelGraph(wdim, 
                                                            _q_input = _q_input, 
                                                            _a_input = _a_input, 
                                                            _q_rcprcl_len_input = _q_rcprcl_len_input, 
                                                            _a_rcprcl_len_input = _a_rcprcl_len_input,
                                                            indep_model = False,
                                                            trainable = False,
                                                            attention_based = False,
                                                            )
        _gated_sim_score = self.gated_sim_mtx_graph.get_score_graph_unit()[0]
        gated_sim_score_len = self.gated_sim_mtx_graph.get_score_graph_unit()[1]
        # siamese convolution layer out dim: (sentence length, feat map num)
        siamese_conv_layer = Conv1D(input_shape = (None, wdim),
                                 filters = feat_map_num_siam,
                                 kernel_size = conv_filter_len_siam,
                                 padding='same',
                                 activation = 'relu',
                                 kernel_regularizer = regularizers.l2(0.00001),
                                 )
        _q_feature_maps_siam = siamese_conv_layer(_q_input)
        _a_feature_maps_siam = siamese_conv_layer(_a_input)
        
        # independent convolution layer 1 out dim: (sentence length_1, feat map num_1)
        # independent convolution layer 2 out dim: (sentence length_2, feat map num_2)
        #_q_feature_maps_indep = Conv1D(input_shape = (None, wdim),
        #                         filters = feat_map_num_1,
        #                         kernel_size = conv_filter_len_1,
        #                         padding='same',
        #                         activation = 'relu',
        #                         kernel_regularizer = regularizers.l2(0.00001),
        #                         )(_q_input)
        #_a_feature_maps_indep = Conv1D(input_shape = (None, wdim),
        #                         filters = feat_map_num_2,
        #                         kernel_size = conv_filter_len_2,
        #                         padding='same',
        #                         activation = 'relu',
        #                         kernel_regularizer = regularizers.l2(0.00001),
        #                         )(_a_input)
                                 
        # siamese pooling res dim: (feat_map_num_1, )
        _q_pooled_maps_siam = GlobalMaxPooling1D()(_q_feature_maps_siam)
        _a_pooled_maps_siam = GlobalMaxPooling1D()(_a_feature_maps_siam)
        
        # q pooling indep res dim: (feat_map_num_1, )
        # a pooling indep res dim: (feat_map_num_2, )
        #_q_pooled_maps_indep = GlobalMaxPooling1D()(_q_feature_maps_indep)
        #_a_pooled_maps_indep = GlobalMaxPooling1D()(_a_feature_maps_indep)
        
        # siamese match
        noise_match_layer = Dense(units = feat_map_num_siam,
                                   activation = None,
                                   use_bias = False,
                                   kernel_regularizer = regularizers.l2(0.0001),
                                   )
        _q_to_a_interm = noise_match_layer(_q_pooled_maps_siam)
        _conv_score = Dot(axes=-1)([_q_to_a_interm, _a_pooled_maps_siam])
        
        # bilateral feature attention
        # feat_1 -> feat_2 attention res dim: (feat_map_2, )
        # feat_2 -> feat_1 attention res dim: (feat_map_1, )
        #attention_layer_1_to_2 = AttentionMatrixLayer(output_dim = feat_map_num_2)
        #attention_layer_2_to_1 = AttentionMatrixLayer(output_dim = feat_map_num_1)
        #attentive_q_to_a_feat = attention_layer_1_to_2(_q_pooled_maps_indep)
        #attentive_a_to_q_feat = attention_layer_2_to_1(_a_pooled_maps_indep)
        
        # norm before dot
        #normed_q_to_a_feat = L2NormLayer()(attentive_q_to_a_feat)
        #normed_a_to_q_feat = L2NormLayer()(attentive_a_to_q_feat)
        #normed_q_feat = L2NormLayer()(_q_pooled_maps_indep)
        #normed_a_feat = L2NormLayer()(_a_pooled_maps_indep)
        # dot matching, res dim: (1, )
        #feat_match_layer = Dot(axes=-1)
        #feat_match_1_to_2 = feat_match_layer([normed_q_to_a_feat, normed_a_feat])
        #feat_match_2_to_1 = feat_match_layer([normed_a_to_q_feat, normed_q_feat])
        
        # distributional similarity, out dim: (1, )
        #_atten_q = TimeDistributed(AttentionMatrixLayer(output_dim=wdim))(_q_input)'''
        
        feed_fwd_lst = [_q_pooled_maps_siam, _a_pooled_maps_siam, _conv_score, _gated_sim_score] # a list of tensors to concatenate
        feed_fwd_tensr_len = feat_map_num_siam * 2 + 1 + gated_sim_score_len # length of matching feature vec
        
        # concatenate res dim: (?, )
        #_conc_res = Concatenate()([_q_pooled_maps_siam, _a_pooled_maps_siam, feat_match_1_to_2, feat_match_2_to_1, _add_feat_input])
        #_conc_res = Concatenate()([feat_match_1_to_2, feat_match_2_to_1, _add_feat_input])
        if len(feed_fwd_lst) > 1:
            _conc_res = Concatenate()(feed_fwd_lst)
        else:
            _conc_res = feed_fwd_lst[0]
        
        _hid_res = Dense(units = feed_fwd_tensr_len, # always in accordance with input
                                 activation = 'tanh',
                                 use_bias = True,
                                 kernel_regularizer = regularizers.l2(0.0001),
                                 )(_conc_res)
        # dropout some units before computing softmax result
        _dropped_hid_res = Dropout(rate=0.5)(_hid_res)
        
        _res = Dense(units = 1,
                     activation = 'sigmoid',
                     use_bias = False,
                     kernel_regularizer = regularizers.l2(0.0001),
                     )(_dropped_hid_res)
        #_res_gate_weight = Dense(units = feed_fwd_tensr_len, # always in accordance with input
        #             activation = 'softmax',
        #             use_bias = False,
        #             kernel_regularizer = regularizers.l2(0.0001),
        #             )(_conc_res)
        #_res = Dot(axes=-1)([_res_gate_weight, _conc_res])
        self.graph_output_unit = _res
        
    def get_model_inputs(self):
        return self.graph_input_units
    
    def get_model_outputs(self):
        return self.graph_output_unit
    
    def set_gate_weights(self, saved_weight_lst):
        self.gated_sim_mtx_graph.set_gate_weights(saved_weight_lst)

class TestModelGraph(object):
    def __init__(self, wdim):
        # input dim: (sentence length, word dim)
        _q_input = Input(shape=(2, wdim))
        #_q_sent_len = _q_input.get_config['batch_input_shape'][1]
        _q_len_input = Input(shape=(1, )) # (1,), every elem: 1 / len(q) || 1 / len(set(q))
        _a_input = Input(shape=(3, wdim))
        #_a_sent_len = _a_input.get_config['batch_input_shape'][1]
        self.input_graph_unit = (_q_input, _q_len_input, _a_input)
        
        _q_vec_normed = L2NormLayer()(_q_input) #(q_len, wdim)
        _a_vec_normed = L2NormLayer()(_a_input)
        # output dim: (q sentence length, a sentence length)
        cos_sim_score_q_a = Dot(axes = -1)([_q_vec_normed, _a_vec_normed]) # (q_len, a_len)
        #atten_tsr = TimeDistributed(Multiply())([_q_vec_normed, _a_vec_normed])
        #sim_mtx_T = Permute((2, 1))(cos_sim_score_q_a)
        atten_mtx_sftmx = Softmax(axis = -2)(cos_sim_score_q_a) # (q_len, a_len)
        atten_ctxt_mtx = Dot(axes = -2)([atten_mtx_sftmx, _q_input])
        
        
        
        '''
        _q_words_best_match = MaxOnASeqLayer()(cos_sim_score_q_a)
        _q_match_score_sum = SumScoreLayer()(_q_words_best_match) # (1, )
        _q_match_score_ave = Multiply()([_q_match_score_sum, _q_len_input]) # (1, )
        
        hid_layer = Dense(units = 1,# + 4,
                         activation = 'linear',
                         kernel_initializer='ones',
                         use_bias = False, #kernel_regularizer = regularizers.l2(0.0001),
                         )
        hid_res = hid_layer(_q_match_score_ave)
        '''
        self.output_graph_unit = atten_ctxt_mtx#hid_res, _q_match_score_ave, _q_match_score_sum, _q_words_best_match, cos_sim_score_q_a
    
    def get_model_inputs(self):
        return self.input_graph_unit
    
    def get_model_outputs(self):
        return self.output_graph_unit
        
